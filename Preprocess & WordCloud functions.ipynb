{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StanfordCoreNLP for processing\n",
    "##### Currently using the tagger only\n",
    "git - https://github.com/smilli/py-corenlp\n",
    "<br>\n",
    "how to run local web server - https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#getting-started\n",
    "<br>\n",
    "on output formats - https://stanfordnlp.github.io/CoreNLP/corenlp-server.html\n",
    "<br><br>\n",
    "Run in cmd to start server\n",
    "<br>\n",
    "cd C:\\stanford-corenlp-full-2017-06-09\n",
    "<br>\n",
    "java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence #1 (6 tokens):\r\n",
      "Bears bear with other bears.\r\n",
      "[Text=Bears CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=NNS]\r\n",
      "[Text=bear CharacterOffsetBegin=6 CharacterOffsetEnd=10 PartOfSpeech=VBP]\r\n",
      "[Text=with CharacterOffsetBegin=11 CharacterOffsetEnd=15 PartOfSpeech=IN]\r\n",
      "[Text=other CharacterOffsetBegin=16 CharacterOffsetEnd=21 PartOfSpeech=JJ]\r\n",
      "[Text=bears CharacterOffsetBegin=22 CharacterOffsetEnd=27 PartOfSpeech=NNS]\r\n",
      "[Text=. CharacterOffsetBegin=27 CharacterOffsetEnd=28 PartOfSpeech=.]\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import re\n",
    "\n",
    "# Initiate CorNLP object\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "output = nlp.annotate(\"Bears bear with other bears.\", properties={\n",
    "                'annotators': 'pos',\n",
    "                'outputFormat': 'text' # json, xml, \n",
    "         })\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "with open(r\"C:\\nlp\\extra_stopwords.txt\", 'r', encoding = 'UTF-8') as f:\n",
    "    extra_stopWords = f.read()\n",
    "    extra_stopWords = extra_stopWords.split(\"\\n\")\n",
    "\n",
    "def preprocess(file_dir):\n",
    "    \n",
    "    ################### Settings ###################\n",
    "    #Stop words\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    stopWords.update(extra_stopWords)\n",
    "    stopWords = list(stopWords)[1:] # spliting by \\n makes whitespace at index 0, get rid of it\n",
    "\n",
    "    # WordNet can take POS tags too\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Title pattern\n",
    "    title = re.compile(r\"%&%.*%&%\")\n",
    "    \n",
    "    # Special chars and tags removal\n",
    "    special_chars = re.compile(r\"[!@##$$%^&*(),:'\\\"]\")\n",
    "    paragraph_tag = re.compile(r\"<p>\")\n",
    "    \n",
    "    # To parse Stanford NLP output\n",
    "    word_pattern = re.compile(r'\\[Text=[A-Za-z]+')\n",
    "    pos_pattern = re.compile(r'=[A-Z]{,4}]')\n",
    "    \n",
    "    # For NLP output\n",
    "    word_tags_list = []\n",
    "    \n",
    "    ################### NLP #######################\n",
    "    # read\n",
    "    text = open(file_dir, 'r', encoding = 'utf-8').read()\n",
    "\n",
    "    # Erase title\n",
    "    text = re.sub(title, \"\", text)\n",
    "    # Special chars\n",
    "    text = re.sub(special_chars, \"\", text)\n",
    "    # Tags\n",
    "    text = re.sub(paragraph_tag, \"\", text)\n",
    "    \n",
    "    # Divide into sentences\n",
    "    sent_tokenize_list = sent_tokenize(text)\n",
    "\n",
    "    # Feed each sentence to parser\n",
    "    for sent in sent_tokenize_list:\n",
    "        sent = sent.lower()\n",
    "        output = nlp.annotate(sent, properties={\n",
    "                    'annotators': 'pos', #                              Options: tokenize, ssplit, pos, lemma, ner, parse, dcoref\n",
    "                    'outputFormat': 'text' # returned as a long string (Options: json, xml, Serialized )\n",
    "             })\n",
    "        parsed_sent = output.split(\"\\r\\n\") # split each token (word & tag)\n",
    "\n",
    "        for i, token in enumerate(parsed_sent[2:]):\n",
    "#             print(\"Token\", i, \":\", token)\n",
    "          \n",
    "            word = re.search(word_pattern, token)\n",
    "            pos = re.search(pos_pattern, token)\n",
    "#             print(\"obj:\", word)\n",
    "            if word != None:\n",
    "                word = word.group()\n",
    "                if pos != None:\n",
    "                    pos = pos.group() \n",
    "                    word = word.replace(\"[Text=\", \"\")\n",
    "                    pos = pos.replace(\"=\", \"\").replace(\"]\", \"\")  \n",
    "#                     print(\"Word:\", word)\n",
    "#                     print(\"POS:\", pos)  \n",
    "                    word_tags_list.append( (word, pos) )\n",
    "                #else: # if word is not alphabetical\n",
    "\n",
    "                #print(\"Word not found:\", word)\n",
    "               # word_tags_list.append( (word, \"N/A\") )\n",
    "\n",
    "    # filters stop words\n",
    "    word_tags_list = [word for word in word_tags_list if word[0] not in stopWords]\n",
    "\n",
    "    # lemmatization\n",
    "#     text = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "\n",
    "    # Join word & tag\n",
    "    word_tags_list = [ wordWithTag[0] + wordWithTag[1]   for wordWithTag in word_tags_list ]\n",
    "\n",
    "    return word_tags_list # list of processed words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get file list from dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10258 text files detected!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = r\"C:\\nlp\\Science-related texts\"\n",
    "file_list = [file for file in os.listdir(directory) if file.endswith('.txt')]\n",
    "\n",
    "print( len(file_list), \"text files detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_cloud(text_string):\n",
    "#     wordcloud = WordCloud().generate(text = text_string)\n",
    "\n",
    "#     plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "    # lower max_font_size\n",
    "    wordcloud = WordCloud(max_font_size=400).generate(text = text_string)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "# For showing in a new window  \n",
    "#     wordcloud = WordCloud().generate(text = text_string)\n",
    "#     image = wordcloud.to_image()\n",
    "#     image.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word cloud for eachc article (control by index num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud for each genre  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4155 academic texts\n",
      " 5781 magazine texts\n",
      " 322 newspaper texts\n"
     ]
    }
   ],
   "source": [
    "acad_files = [file for file in file_list if file.startswith(\"acad\")]\n",
    "mag_files = [file for file in file_list if file.startswith(\"mag\")]\n",
    "news_files = [file for file in file_list if file.startswith(\"news\")]\n",
    "\n",
    "print( len(acad_files) , \"academic texts\\n\",\n",
    "       len(mag_files) , \"magazine texts\\n\",\n",
    "       len(news_files) , \"newspaper texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Academic genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-283-eb834c3bc2b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0macad_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mall_text\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-278-f43d41fb10c7>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(file_dir)\u001b[0m\n\u001b[0;32m     53\u001b[0m         output = nlp.annotate(sent, properties={\n\u001b[0;32m     54\u001b[0m                     \u001b[1;34m'annotators'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'pos'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#                              Options: tokenize, ssplit, pos, lemma, ner, parse, dcoref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                     \u001b[1;34m'outputFormat'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'text'\u001b[0m \u001b[1;31m# returned as a long string (Options: json, xml, Serialized )\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m              })\n\u001b[0;32m     57\u001b[0m         \u001b[0mparsed_sent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r\\n\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# split each token (word & tag)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\nlp\\lib\\site-packages\\pycorenlp\\corenlp.py\u001b[0m in \u001b[0;36mannotate\u001b[1;34m(self, text, properties)\u001b[0m\n\u001b[0;32m     27\u001b[0m             self.server_url, params={\n\u001b[0;32m     28\u001b[0m                 \u001b[1;34m'properties'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproperties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             }, data=data, headers={'Connection': 'close'})\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         if ('outputFormat' in properties\n",
      "\u001b[1;32mc:\\nlp\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \"\"\"\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\nlp\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\nlp\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    506\u001b[0m         }\n\u001b[0;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\nlp\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\nlp\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 )\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\nlp\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\nlp\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;31m# Reset the timeout for the recv() on the socket\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ch\\appdata\\local\\programs\\python\\python36\\Lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                 encode_chunked=False):\n\u001b[0;32m   1238\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ch\\appdata\\local\\programs\\python\\python36\\Lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1283\u001b[0m             \u001b[1;31m# default charset of iso-8859-1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ch\\appdata\\local\\programs\\python\\python36\\Lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1232\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[1;32mc:\\users\\ch\\appdata\\local\\programs\\python\\python36\\Lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1024\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"\\r\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ch\\appdata\\local\\programs\\python\\python36\\Lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\nlp\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\nlp\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             conn = connection.create_connection(\n\u001b[1;32m--> 141\u001b[1;33m                 (self.host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\nlp\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mfamily\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallowed_gai_family\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0msock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ch\\appdata\\local\\programs\\python\\python36\\Lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    741\u001b[0m     \u001b[1;31m# and socket type values to enum constants.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    744\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m         addrlist.append((_intenum_converter(af, AddressFamily),\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "all_text = \"\"\n",
    "\n",
    "for file in acad_files:\n",
    "    text = preprocess(os.path.join(directory, file))\n",
    "    processed = ' '.join(text)\n",
    "    all_text += processed \n",
    "    \n",
    "print(\"Academic texts -\", len(all_text), \"words\")\n",
    "\n",
    "draw_cloud(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magazine genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "all_text = \"\"\n",
    "\n",
    "for file in mag_files:\n",
    "    text = preprocess(os.path.join(directory, file))\n",
    "    processed = ' '.join(text)\n",
    "    all_text += processed \n",
    "    \n",
    "print(\"Magazine texts -\", len(all_text), \"words\")\n",
    "\n",
    "draw_cloud(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newspaper genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "all_text = \"\"\n",
    "\n",
    "for file in news_files:\n",
    "    text = preprocess(os.path.join(directory, file))\n",
    "    processed = ' '.join(text)\n",
    "    all_text += processed \n",
    "\n",
    "print(\"Newspaper texts -\", len(all_text), \"words\")\n",
    "\n",
    "draw_cloud(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * In case of converting list to counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sunlight': 4, 'central': 1, 'source': 7, 'united': 3, 'state': 4, 'energy': 24, 'st': 1, 'century': 2, 'sprawling': 1, 'low': 1, 'rise': 1, 'campus': 1, 'solar': 11, 'research': 7, 'institute': 2, 'seri': 6, 'answer': 2, 'resounding': 1, 'yes': 1, 'politically': 1, 'may': 1, 'coming': 2, 'time': 1, 'conventional': 4, 'power': 13, 'generation': 1, 'coal': 2, 'oil': 6, 'natural': 1, 'gas': 6, 'increasing': 1, 'fire': 1, 'environmentally': 2, 'conscious': 1, 'public': 2, 'concerned': 1, 'global': 2, 'warming': 2, 'acid': 1, 'rain': 1, 'air': 2, 'pollution': 2, 'also': 2, 'concern': 1, 'national': 1, 'security': 1, 'implication': 1, 'dependence': 1, 'nearly': 3, 'percent': 10, 'imported': 1, 'nuclear': 7, 'touted': 1, 'clean': 1, 'alternative': 5, 'suffering': 1, 'continuing': 2, 'revelation': 1, 'following': 2, 'explosion': 1, 'soviet': 1, 'reactor': 1, 'chernobyl': 1, 'station': 2, 'political': 1, 'difficulty': 1, 'surrounding': 1, 'start': 1, 'seabrook': 1, 'plant': 7, 'new': 2, 'hampshire': 1, 'seen': 1, 'many': 4, 'portent': 1, 'greater': 1, 'challenge': 2, 'facing': 1, 'installation': 1, 'future': 1, 'fill': 1, 'gap': 1, 'say': 14, 'robert': 1, 'stokes': 3, 'deputy': 1, 'director': 1, 'year': 4, 'old': 1, 'facility': 1, 'owned': 1, 'u': 2, 'department': 1, 'currently': 1, 'budgeted': 1, 'million': 1, 'believe': 1, 'government': 1, 'set': 1, 'course': 1, 'gradually': 1, 'increase': 1, 'development': 2, 'investment': 2, 'renewable': 2, 'point': 1, 'technology': 13, 'become': 2, 'cost': 6, 'effective': 3, 'mr': 2, 'researcher': 3, 'taking': 1, 'expanded': 1, 'view': 1, 'word': 1, 'use': 2, 'cover': 2, 'resource': 5, 'ultimately': 2, 'derive': 1, 'sun': 3, 'including': 2, 'wind': 5, 'combustion': 1, 'organic': 1, 'biomass': 3, 'product': 1, 'wave': 1, 'tidal': 1, 'thermal': 3, 'gradient': 1, 'ocean': 1, 'number': 2, 'already': 8, 'certain': 2, 'application': 2, 'much': 3, 'closer': 1, 'today': 4, 'eight': 1, 'ago': 1, 'eventual': 1, 'economic': 1, 'parity': 1, 'variety': 1, 'tom': 1, 'bath': 4, 'manager': 1, 'photovoltaics': 2, 'pv': 4, 'direct': 2, 'conversion': 6, 'electricity': 3, 'using': 1, 'solid': 2, 'photo': 1, 'cell': 2, 'powering': 1, 'satellite': 1, 'since': 1, 'vanguard': 1, 'launched': 1, 'choice': 1, 'small': 1, 'far': 1, 'grid': 2, 'rural': 1, 'water': 3, 'pump': 1, 'communication': 1, 'relay': 1, 'vaccine': 1, 'refrigerator': 1, 'third': 2, 'world': 2, 'nation': 6, 'even': 1, 'competitive': 2, 'remote': 1, 'home': 1, 'mile': 1, 'dr': 2, 'pay': 1, 'buy': 1, 'battery': 2, 'rather': 1, 'try': 1, 'hook': 1, 'like': 1, 'luz': 2, 'international': 1, 'kramer': 1, 'junction': 1, 'calif': 2, 'acre': 1, 'trough': 1, 'shaped': 1, 'mirror': 1, 'focus': 1, 'ray': 1, 'vacuum': 1, 'insulated': 1, 'tube': 1, 'heated': 1, 'degree': 1, 'used': 1, 'generate': 2, 'superheated': 1, 'steam': 1, 'drive': 2, 'turbine': 3, 'generator': 1, 'le': 1, 'cent': 2, 'per': 2, 'kilowatt': 3, 'hour': 3, 'official': 1, 'cheaper': 1, 'producing': 2, 'megawatt': 2, 'southern': 1, 'california': 2, 'reach': 1, 'almost': 1, 'enough': 1, 'meet': 2, 'entire': 1, 'residential': 1, 'need': 5, 'city': 1, 'size': 1, 'san': 1, 'francisco': 1, 'phoenix': 1, 'typically': 1, 'generated': 1, 'windmill': 1, 'farm': 1, 'located': 2, 'high': 1, 'mountain': 1, 'pass': 1, 'west': 1, 'viable': 1, 'industry': 1, 'alone': 2, 'billion': 2, 'medium': 1, 'sized': 1, 'nonpolluting': 1, 'paul': 1, 'gipe': 1, 'american': 1, 'association': 1, 'tehachapi': 1, 'offset': 1, 'pound': 1, 'greenhouse': 2, 'would': 1, 'otherwise': 1, 'pour': 1, 'atmosphere': 1, 'take': 1, 'form': 1, 'wood': 1, 'burning': 1, 'municipal': 1, 'waste': 1, 'supplying': 1, 'potential': 2, 'complex': 1, 'biological': 1, 'crop': 1, 'alcohol': 1, 'fuel': 3, 'biogas': 1, 'cultivation': 1, 'microalgae': 1, 'oilseed': 1, 'synthetic': 1, 'limited': 3, 'operation': 1, 'taken': 1, 'together': 1, 'could': 2, 'produce': 3, 'hydrogen': 5, 'vapor': 1, 'burn': 1, 'increasingly': 1, 'considered': 1, 'transportation': 1, 'mean': 1, 'store': 1, 'easily': 1, 'made': 2, 'electrolysis': 1, 'process': 2, 'us': 1, 'current': 1, 'split': 1, 'molecule': 1, 'oxygen': 1, 'atom': 1, 'cheap': 1, 'economically': 1, 'attractive': 1, 'powered': 1, 'automobile': 1, 'prototype': 1, 'existence': 1, 'recent': 1, 'report': 1, 'washington': 1, 'note': 1, 'turn': 1, 'photovoltaic': 2, 'fall': 1, 'sunnier': 1, 'region': 2, 'bottom': 1, 'line': 1, 'depends': 1, 'whether': 1, 'remains': 1, 'constant': 1, 'go': 1, 'mix': 2, 'around': 1, 'intensify': 1, 'get': 1, 'figure': 1, 'roughly': 1, 'equivalent': 1, 'acknowledge': 1, 'quiet': 1, 'period': 1, 'history': 1, 'flurry': 1, 'interest': 1, 'arab': 1, 'embargo': 1, 'led': 1, 'upsurge': 1, 'price': 1, 'fell': 1, 'however': 4, 'urgency': 1, 'develop': 1, 'declined': 1, 'leaving': 1, 'reputation': 1, 'kind': 1, 'idealistic': 1, 'crusade': 1, 'last': 1, 'decade': 1, 'steady': 1, 'progress': 2, 'front': 1, 'example': 1, 'available': 2, 'efficient': 1, 'converting': 1, 'contrast': 2, 'nature': 1, 'long': 1, 'term': 1, 'horizon': 1, 'barely': 1, 'scratched': 1, 'kenneth': 1, 'zweibel': 2, 'program': 2, 'efficiency': 1, 'matures': 1, 'fact': 1, 'maturing': 1, 'success': 1, 'system': 2, 'generally': 1, 'add': 1, 'case': 1, 'diffuse': 2, 'erratic': 1, 'two': 1, 'factor': 1, 'provide': 1, 'technological': 1, 'reason': 1, 'work': 1, 'well': 1, 'sort': 1, 'usually': 1, 'cloud': 1, 'northeast': 1, 'adequate': 1, 'southwest': 1, 'storage': 1, 'superconducting': 1, 'ring': 1, 'watching': 1, 'rate': 1, 'optimistic': 1, 'flatly': 1, 'crisis': 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = dict(Counter(text))\n",
    "print(counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
