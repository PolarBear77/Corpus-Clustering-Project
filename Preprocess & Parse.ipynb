{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StanfordCoreNLP parser\n",
    "##### Currently using the parser only, add NER later\n",
    "git - https://github.com/smilli/py-corenlp\n",
    "<br>\n",
    "how to run local web server - https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#getting-started\n",
    "<br>\n",
    "on output formats - https://stanfordnlp.github.io/CoreNLP/corenlp-server.html\n",
    "<br><br>\n",
    "Run in cmd to start server\n",
    "<br>\n",
    "cd C:\\stanford-corenlp-full-2017-06-09\n",
    "<br>\n",
    "java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API server setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence #1 (6 tokens):\r\n",
      "Bears bear with other bears.\r\n",
      "[Text=Bears CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=NNS]\r\n",
      "[Text=bear CharacterOffsetBegin=6 CharacterOffsetEnd=10 PartOfSpeech=VBP]\r\n",
      "[Text=with CharacterOffsetBegin=11 CharacterOffsetEnd=15 PartOfSpeech=IN]\r\n",
      "[Text=other CharacterOffsetBegin=16 CharacterOffsetEnd=21 PartOfSpeech=JJ]\r\n",
      "[Text=bears CharacterOffsetBegin=22 CharacterOffsetEnd=27 PartOfSpeech=NNS]\r\n",
      "[Text=. CharacterOffsetBegin=27 CharacterOffsetEnd=28 PartOfSpeech=.]\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import re\n",
    "\n",
    "# Initiate CorNLP object\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "output = nlp.annotate(\"Bears bear with other bears.\", properties={\n",
    "                'annotators': 'pos',\n",
    "                'outputFormat': 'text' # json, xml, \n",
    "         })\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw text preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "def preprocess(file_dir):\n",
    "    \"\"\"\n",
    "    Removes special chars , title\n",
    "    Normalizes spaces>2  to one\n",
    "    \"\"\"\n",
    "\n",
    "    title = re.compile(r\"%&%.*%&%\")\n",
    "    special_chars = re.compile(r\"[!@##$$%^&*(),:\\\"]\") \n",
    "    parag_tag = re.compile(\"<p>\")\n",
    "    \n",
    "    text = open(file_dir, 'r', encoding = 'utf-8').read()\n",
    "    \n",
    "    text = re.sub(title, \"\", text)\n",
    "    text = re.sub(special_chars, \"\", text)\n",
    "    text = re.sub(parag_tag, \"\", text)\n",
    "    text = re.sub(\"\\s{2,}\", \" \",text) # Normalize 2 > whitespace to 1 whitespace\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag, acceptPeriods = False): \n",
    "    \"\"\"\n",
    "    Changes treebank tags to Wordnet tags to be fed into the WordNet lemmatizer\n",
    "    Returns -1 except for ADJ, VERB, NOUN, ADV\n",
    "    \n",
    "    periods can be accepted with acceptPeriods\n",
    "    tags for periods are returned as 1\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "#     elif treebank_tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "    elif (acceptPeriods == True )& (treebank_tag == \".\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(text, show_pos = True):\n",
    "    \"\"\"\n",
    "    Parses text via the Stanford parser, filters outuput based on POS tags to be fed into lemmatizer   \n",
    "    Removes stop words from the output\n",
    "    \n",
    "    Params\n",
    "    show_pos = returns POS tags with words as a pair\n",
    "    \n",
    "    API parameters for Standford parser\n",
    "    annotators: tokenize, ssplit, pos, lemma, ner, parse, dcoref\n",
    "    outputFormats: text, json, xml, Serialized\n",
    "    \"\"\"              \n",
    "    \n",
    "    output = nlp.annotate(text, properties={\n",
    "            'annotators': 'ssplit, pos', \n",
    "            'outputFormat': 'json'\n",
    "            })\n",
    "    \n",
    "    word_tags_list = []\n",
    "    for sentence in output['sentences']:\n",
    "        for item in sentence['tokens']:\n",
    "            \n",
    "            word = item['word']\n",
    "            pos = item['pos']\n",
    "            \n",
    "            # Append to list\n",
    "            if (show_pos == False):\n",
    "                word_tags_list.append(word)\n",
    "            elif (show_pos == True):\n",
    "                word_tags_list.append(word + \"_\"+ pos)\n",
    "            \n",
    "    return word_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I_PRP', 'like_VBP', 'your_PRP$', 'friend_NN']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_text(\"I like your friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatizer(listOfWords, pos = [], acceptPeriods = False):\n",
    "    \"\"\"\n",
    "    Takes list of words and pos and returns a list of lemmas\n",
    "    A list of POS tags can be supplied with words to be fed into WordNet lemmatizer \n",
    "    param acceptPeriods will return periods as periods\n",
    "    \"\"\"\n",
    "    wordNet = WordNetLemmatizer()\n",
    "\n",
    "    lemmatized_list = []\n",
    "\n",
    "    if len(pos) == 0:\n",
    "        for word in listOfWords:\n",
    "            lemmatized_list.append(wordNet.lemmatize(word))\n",
    "        return lemmatized_list\n",
    "\n",
    "    else: # if pos list was given\n",
    "        for i, word in enumerate(listOfWords):\n",
    "            if (get_wordnet_pos(pos[i]) == -1):\n",
    "                continue\n",
    "            if (get_wordnet_pos(pos[i]) == 1):\n",
    "                lemmatized_list.append\n",
    "            elif (get_wordnet_pos(pos[i]) != -1):\n",
    "                word = wordNet.lemmatize(word, get_wordnet_pos(pos[i]))\n",
    "                lemmatized_list.append(word)\n",
    "                \n",
    "        return lemmatized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def removeStopWords(listOfWords):\n",
    "    stopWords = []\n",
    "    stopWords = stopwords.words('English')\n",
    "    with open(r\"C:\\nlp\\extra_stopwords.txt\", 'r', encoding = 'UTF-8') as f:\n",
    "        extra_stopWords = f.read()\n",
    "        extra_stopWords = extra_stopWords.split(\"\\n\")\n",
    "        stopWords.append(extra_stopWords)\n",
    "            \n",
    "    listOfWords = [word for word in listOfWords if word not in stopWords]\n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show difference w/ and w/o lemmatize \n",
    "- notice plural words and verbs lemmatzied to their original form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "directory = r\"C:\\nlp\\Science-related texts\"\n",
    "file_list = [file for file in os.listdir(directory) if file.endswith('.txt')]\n",
    "\n",
    "# Use 1st file as example\n",
    "temp_dir = directory+\"\\\\\"+file_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e41c2c52535a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mlemmatized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-d947b4e99572>\u001b[0m in \u001b[0;36mlemmatizer\u001b[1;34m(listOfWords, pos)\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mget_wordnet_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                 \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m                 \u001b[0mlemmatized_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\nlp\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\nlp\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   1766\u001b[0m         \u001b[1;31m#    find a match or you can't go any further\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m         \u001b[0mexceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1769\u001b[0m         \u001b[0msubstitutions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMORPHOLOGICAL_SUBSTITUTIONS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '.'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "temp = preprocess(temp_dir)\n",
    "parsed = parse_text(temp)\n",
    "\n",
    "temp = [item.split(\"_\") for item in parsed]\n",
    "words = [ item[0] for item in temp]\n",
    "pos = [ item[1] for item in temp]\n",
    "\n",
    "lemmatized = lemmatizer(words, pos)\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "periods = (i for i, word in enumerate(words) if word == '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('.','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
